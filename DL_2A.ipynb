{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#loading imdb data with most frequent 10000 words\n",
    "from keras.datasets import imdb\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000) # you may take top 10,000\n",
    "word frequently used review of movies other are discarded\n",
    "#consolidating data for EDA Exploratory data analysis (EDA) is used by data scientists to analyze and\n",
    "investigate data sets and summarize their main characteristics\n",
    "data = np.concatenate((X_train, X_test), axis=0) # axis 0 is first running vertically downwards across\n",
    "rows (axis 0), axis 1 is second running horizontally across columns (axis 1),\n",
    "label = np.concatenate((y_train, y_test), axis=0)\n",
    "X_train.shape\n",
    "(25000,)\n",
    "X_test.shape\n",
    "(25000,)\n",
    "y_train.shape\n",
    "(25000,)\n",
    "y_test.shape\n",
    "(25000,)\n",
    "print(\"Review is \",X_train[0]) # series of no converted word to vocabulory associated with index\n",
    "print(\"Review is \",y_train[0])\n",
    "Review is [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14,\n",
    "394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114,\n",
    "9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5,\n",
    "89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4,\n",
    "1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165,\n",
    "4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255,\n",
    "5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64,\n",
    "1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
    "Review is 0\n",
    "vocab=imdb.get_word_index() # Retrieve the word index file mapping words to indices\n",
    "print(vocab)\n",
    "\n",
    "y_train\n",
    "array([1, 0, 0, ..., 0, 1, 0])\n",
    "y_test\n",
    "array([0, 1, 1, ..., 0, 0, 0])\n",
    "# Function to perform relevant sequence adding on the data\n",
    "# Now it is time to prepare our data. We will vectorize every review and fill it with zeros so that it\n",
    "\n",
    "# \fcontains exactly 10000 numbers.\n",
    "# # That means we fill every review that is shorter than 500 with zeros.\n",
    "# # We do this because the biggest review is nearly that long and every input for our neural network needs\n",
    "# to have the same size.\n",
    "# We also transform the targets into floats.\n",
    "# sequences is name of method the review less than 10000 we perform padding overthere\n",
    "# binary vectorization code:\n",
    "# VECTORIZE as one cannot feed integers into a NN\n",
    "# Encoding the integer sequences into a binary matrix - one hot encoder basically\n",
    "# From integers representing words, at various lengths - to a normalized one hot encoded tensor (matrix)\n",
    "#of 10k columns\n",
    "def vectorize(sequences, dimension = 10000):\n",
    "\n",
    "# We will vectorize every review and fill it with zeros\n",
    "\n",
    "so that it contains exactly 10,000 numbers.\n",
    "# Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "results = np.zeros((len(sequences), dimension))\n",
    "for i, sequence in enumerate(sequences):\n",
    "results[i, sequence] = 1\n",
    "return results\n",
    "# Now we split our data into a training and a testing set.\n",
    "# The training set will contain reviews and the testing set\n",
    "# # Set a VALIDATION set\n",
    "test_x = data[:10000]\n",
    "test_y = label[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = label[10000:]\n",
    "test_x.shape\n",
    "(10000,)\n",
    "test_y.shape\n",
    "(10000,)\n",
    "train_x.shape\n",
    "(40000,)\n",
    "train_y.shape\n",
    "(40000,)\n",
    "print(\"Categories:\", np.unique(label))\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
    "\n",
    "# The hstack() function is used to stack arrays in sequence horizontally (column wise).\n",
    "Categories: [0 1]\n",
    "Number of unique words: 9998\n",
    "length = [len(i) for i in data]\n",
    "print(\"Average Review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))\n",
    "# The whole dataset contains 9998 unique words and the average review length is 234 words, with a\n",
    "standard deviation of 173 words.\n",
    "Average Review length: 234.75892\n",
    "Standard Deviation: 173\n",
    "# If you look at the data you will realize it has been already pre-processed.\n",
    "# All words have been mapped to integers and the integers represent the words sorted by their frequency.\n",
    "# This is very common in text analysis to represent a dataset like this.\n",
    "# So 4 represents the 4th most used word,\n",
    "# 5 the 5th most used word and so on...\n",
    "# The integer 1 is reserved for the start marker,\n",
    "# the integer 2 for an unknown word and 0 for padding.\n",
    "# Let's look at a single training example:\n",
    "print(\"Label:\", label[0])\n",
    "Label: 1\n",
    "print(\"Label:\", label[1])\n",
    "Label: 0\n",
    "print(data[0])\n",
    "# Retrieves a dict mapping words to their index in the IMDB dataset.\n",
    "index = imdb.get_word_index() # word to index\n",
    "# Create inverted index from a dictionary with document ids as keys and a list of terms as values for\n",
    "each document\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) # id to word\n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
    "# The indices are offset by 3 because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\"\n",
    "and \"unknown\".\n",
    "print(decoded)\n",
    "# this film was just brilliant casting location scenery story direction everyone's really suited the p # is an amazing actor and now the same being# father came from the same scottish island as myself so i loved the fact there wasith this film the witty remarks throughout the film\n",
    "#Adding sequence to data\n",
    "# Vectorization is the process of converting textual data into numerical vectors and is a process that is\n",
    "usually applied once the text is cleaned.\n",
    "data = vectorize(data)\n",
    "label = np.array(label).astype(\"float32\")\n",
    "labelDF=pd.DataFrame({'label':label})\n",
    "sns.countplot(x='label', data=labelDF)\n",
    "\n",
    "# Creating train and test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,label, test_size=0.20, random_state=1)\n",
    "X_train.shape\n",
    "(40000, 10000)\n",
    "X_test.shape\n",
    "(10000, 10000)\n",
    "# Let's create sequential model\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "# Note that we set the input-shape to 10,000 at the input-layer because our reviews are 10,000 integers\n",
    "long.\n",
    "# The input-layer takes 10,000 as input and outputs it with a shape of 50.\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
    "\n",
    "# Hidden - Layers\n",
    "# Please note you should always use a dropout rate between 20% and 50%. # here in our case 0.3 means\n",
    "30% dropout we are using dropout to prevent overfitting.\n",
    "# By the way, if you want you can build a sentiment analysis without LSTMs, then you simply need to\n",
    "replace it by a flatten layer:\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "Model: \"sequential\"\n",
    "Layer (type)\n",
    "\n",
    "Output Shape\n",
    "\n",
    "Param #\n",
    "\n",
    "=================================================================\n",
    "dense (Dense)\n",
    "\n",
    "(None, 50)\n",
    "\n",
    "500050\n",
    "\n",
    "dropout (Dropout)\n",
    "\n",
    "(None, 50)\n",
    "\n",
    "0\n",
    "\n",
    "dense_1 (Dense)\n",
    "\n",
    "(None, 50)\n",
    "\n",
    "2550\n",
    "\n",
    "dropout_1 (Dropout)\n",
    "\n",
    "(None, 50)\n",
    "\n",
    "0\n",
    "\n",
    "dense_2 (Dense)\n",
    "\n",
    "(None, 50)\n",
    "\n",
    "2550\n",
    "\n",
    "dense_3 (Dense)\n",
    "\n",
    "(None, 1)\n",
    "\n",
    "51\n",
    "\n",
    "=================================================================\n",
    "Total params: 505,201\n",
    "Trainable params: 505,201\n",
    "Non-trainable params: 0\n",
    "#For early stopping\n",
    "# Stop training when a monitored metric has stopped improving.\n",
    "# monitor: Quantity to be monitored.\n",
    "# patience: Number of epochs with no improvement after which training will be\n",
    "stopped.\n",
    "import tensorflow as tf\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "# We use the “adam” optimizer, an algorithm that changes the weights and biases\n",
    "during training.\n",
    "#\n",
    "\n",
    "We\n",
    "\n",
    "also\n",
    "\n",
    "choose\n",
    "\n",
    "binary-crossentropy as loss (because we deal with binary\n",
    "\n",
    "classification) and accuracy as our evaluation metric.\n",
    "model.compile(\n",
    "optimizer = \"adam\",\n",
    "loss = \"binary_crossentropy\",\n",
    "metrics = [\"accuracy\"]\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "results = model.fit(\n",
    "X_train, y_train,\n",
    "epochs= 2,\n",
    "batch_size = 500,\n",
    "validation_data = (X_test, y_test),\n",
    "callbacks=[callback]\n",
    ")\n",
    "# Let's check mean accuracy of our model\n",
    "print(np.mean(results.history[\"val_accuracy\"]))\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, batch_size=500)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "20/20 [==============================] - 1s 24ms/step - loss: 0.2511 - accuracy:\n",
    "0.8986\n",
    "Test loss: 0.25108325481414795\n",
    "Test accuracy: 0.8985999822616577\n",
    "#Let's plot training history of our model.\n",
    "# list all data in history\n",
    "print(results.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
